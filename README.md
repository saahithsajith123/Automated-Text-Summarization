This conference paper offers an extensive examination of automated text summarization techniques, spanning from traditional methods like TextRank to state-of-the-art models such as GPT 2, GPT-3, and BERT. Text summarization stands as a pivotal task in numerous domains, serving to condense large volumes of text while preserving essential information. However, it faces inherent challenges, including the risk of information loss and the necessity of maintaining coherence. Through a meticulous comparative analysis, we delve into the efficacy, computational efficiency, and real-world applicability of these summarization techniques. Our evaluation encompasses diverse factors such as summarization quality, language fluency, and semantic comprehension. Moreover, we investigate the impact of fine-tuning and parameter optimization on the summarization process. By shedding light on these aspects, our findings aim to empower researchers and practitioners in natural language processing, enabling them to make informed decisions and advancements in text summarization applications. Keywordsâ€” automated text summarization, GPT-2, GPT-3, BERT, TextRank, summarization quality, language fluency, semantic understanding, parameter optimization.
